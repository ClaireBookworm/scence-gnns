<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_3yo7njaol6n6-2>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-2}.lst-kix_53gvqnbfbcso-4>li:before{content:"-  "}.lst-kix_53gvqnbfbcso-3>li:before{content:"-  "}.lst-kix_53gvqnbfbcso-2>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-0.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-0 0}.lst-kix_53gvqnbfbcso-0>li:before{content:"-  "}.lst-kix_3yo7njaol6n6-6>li:before{content:"" counter(lst-ctn-kix_3yo7njaol6n6-6,lower-latin) ". "}.lst-kix_53gvqnbfbcso-1>li:before{content:"-  "}.lst-kix_3yo7njaol6n6-1>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-1}.lst-kix_3yo7njaol6n6-7>li:before{content:"" counter(lst-ctn-kix_3yo7njaol6n6-7,lower-roman) ". "}.lst-kix_3yo7njaol6n6-8>li:before{content:"" counter(lst-ctn-kix_3yo7njaol6n6-8,decimal) ". "}ul.lst-kix_53gvqnbfbcso-4{list-style-type:none}ul.lst-kix_53gvqnbfbcso-5{list-style-type:none}ul.lst-kix_53gvqnbfbcso-2{list-style-type:none}ul.lst-kix_53gvqnbfbcso-3{list-style-type:none}ul.lst-kix_53gvqnbfbcso-0{list-style-type:none}ul.lst-kix_53gvqnbfbcso-1{list-style-type:none}ol.lst-kix_3yo7njaol6n6-8.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-8 0}ul.lst-kix_53gvqnbfbcso-8{list-style-type:none}ul.lst-kix_53gvqnbfbcso-6{list-style-type:none}ul.lst-kix_53gvqnbfbcso-7{list-style-type:none}ul.lst-kix_et8j17tmg5fs-4{list-style-type:none}.lst-kix_et8j17tmg5fs-5>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-3{list-style-type:none}ul.lst-kix_et8j17tmg5fs-5{list-style-type:none}ol.lst-kix_3yo7njaol6n6-4{list-style-type:none}ul.lst-kix_et8j17tmg5fs-2{list-style-type:none}.lst-kix_et8j17tmg5fs-4>li:before{content:"-  "}.lst-kix_et8j17tmg5fs-6>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-1{list-style-type:none}ul.lst-kix_et8j17tmg5fs-3{list-style-type:none}ol.lst-kix_3yo7njaol6n6-2{list-style-type:none}ul.lst-kix_et8j17tmg5fs-0{list-style-type:none}.lst-kix_et8j17tmg5fs-3>li:before{content:"-  "}.lst-kix_et8j17tmg5fs-7>li:before{content:"-  "}ul.lst-kix_et8j17tmg5fs-1{list-style-type:none}ol.lst-kix_3yo7njaol6n6-0{list-style-type:none}ul.lst-kix_o66zyiiwgmva-8{list-style-type:none}ul.lst-kix_o66zyiiwgmva-7{list-style-type:none}.lst-kix_et8j17tmg5fs-1>li:before{content:"-  "}.lst-kix_et8j17tmg5fs-0>li:before{content:"-  "}.lst-kix_et8j17tmg5fs-2>li:before{content:"-  "}.lst-kix_et8j17tmg5fs-8>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-7{list-style-type:none}ol.lst-kix_3yo7njaol6n6-8{list-style-type:none}ol.lst-kix_3yo7njaol6n6-5{list-style-type:none}ol.lst-kix_3yo7njaol6n6-6{list-style-type:none}.lst-kix_f6w07r8prvk9-2>li:before{content:"-  "}.lst-kix_f6w07r8prvk9-3>li:before{content:"-  "}.lst-kix_f6w07r8prvk9-4>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-2.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-2 0}.lst-kix_f6w07r8prvk9-6>li:before{content:"-  "}.lst-kix_f6w07r8prvk9-5>li:before{content:"-  "}.lst-kix_f6w07r8prvk9-7>li:before{content:"-  "}ul.lst-kix_f6w07r8prvk9-0{list-style-type:none}.lst-kix_n23ztf2yvbsp-3>li:before{content:"-  "}ul.lst-kix_f6w07r8prvk9-1{list-style-type:none}ul.lst-kix_f6w07r8prvk9-2{list-style-type:none}.lst-kix_n23ztf2yvbsp-4>li:before{content:"-  "}.lst-kix_n23ztf2yvbsp-6>li:before{content:"-  "}.lst-kix_n23ztf2yvbsp-5>li:before{content:"-  "}.lst-kix_n23ztf2yvbsp-7>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-3.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-3 0}.lst-kix_3yo7njaol6n6-0>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-0}.lst-kix_53gvqnbfbcso-5>li:before{content:"-  "}.lst-kix_f6w07r8prvk9-1>li:before{content:"-  "}.lst-kix_3yo7njaol6n6-3>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-3}.lst-kix_53gvqnbfbcso-6>li:before{content:"-  "}.lst-kix_n23ztf2yvbsp-8>li:before{content:"-  "}.lst-kix_f6w07r8prvk9-0>li:before{content:"-  "}.lst-kix_3yo7njaol6n6-6>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-6}.lst-kix_53gvqnbfbcso-8>li:before{content:"-  "}ul.lst-kix_f6w07r8prvk9-7{list-style-type:none}ul.lst-kix_o66zyiiwgmva-6{list-style-type:none}ul.lst-kix_f6w07r8prvk9-8{list-style-type:none}ul.lst-kix_o66zyiiwgmva-5{list-style-type:none}.lst-kix_53gvqnbfbcso-7>li:before{content:"-  "}ul.lst-kix_o66zyiiwgmva-4{list-style-type:none}ul.lst-kix_o66zyiiwgmva-3{list-style-type:none}ul.lst-kix_et8j17tmg5fs-8{list-style-type:none}ul.lst-kix_f6w07r8prvk9-3{list-style-type:none}ul.lst-kix_o66zyiiwgmva-2{list-style-type:none}ul.lst-kix_f6w07r8prvk9-4{list-style-type:none}ul.lst-kix_o66zyiiwgmva-1{list-style-type:none}ul.lst-kix_et8j17tmg5fs-6{list-style-type:none}ul.lst-kix_f6w07r8prvk9-5{list-style-type:none}ul.lst-kix_o66zyiiwgmva-0{list-style-type:none}ul.lst-kix_et8j17tmg5fs-7{list-style-type:none}ul.lst-kix_f6w07r8prvk9-6{list-style-type:none}ol.lst-kix_3yo7njaol6n6-4.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-4 0}.lst-kix_3yo7njaol6n6-8>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-8}.lst-kix_o66zyiiwgmva-7>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-8>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-7.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-7 0}.lst-kix_3yo7njaol6n6-7>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-7}.lst-kix_n23ztf2yvbsp-2>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-0>li:before{content:"-  "}.lst-kix_n23ztf2yvbsp-1>li:before{content:"-  "}.lst-kix_n23ztf2yvbsp-0>li:before{content:"-  "}.lst-kix_f6w07r8prvk9-8>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-1>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-6>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-5>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-3>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-4>li:before{content:"-  "}.lst-kix_o66zyiiwgmva-2>li:before{content:"-  "}ol.lst-kix_3yo7njaol6n6-1.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-1 0}ul.lst-kix_n23ztf2yvbsp-8{list-style-type:none}ul.lst-kix_n23ztf2yvbsp-4{list-style-type:none}ul.lst-kix_n23ztf2yvbsp-5{list-style-type:none}ul.lst-kix_n23ztf2yvbsp-6{list-style-type:none}ul.lst-kix_n23ztf2yvbsp-7{list-style-type:none}.lst-kix_3yo7njaol6n6-5>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-5}ul.lst-kix_n23ztf2yvbsp-0{list-style-type:none}.lst-kix_3yo7njaol6n6-4>li{counter-increment:lst-ctn-kix_3yo7njaol6n6-4}ul.lst-kix_n23ztf2yvbsp-1{list-style-type:none}ul.lst-kix_n23ztf2yvbsp-2{list-style-type:none}ul.lst-kix_n23ztf2yvbsp-3{list-style-type:none}ol.lst-kix_3yo7njaol6n6-5.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-5 0}.lst-kix_3yo7njaol6n6-5>li:before{content:"(" counter(lst-ctn-kix_3yo7njaol6n6-5,decimal) ") "}.lst-kix_3yo7njaol6n6-4>li:before{content:"(" counter(lst-ctn-kix_3yo7njaol6n6-4,lower-roman) ") "}.lst-kix_3yo7njaol6n6-2>li:before{content:"" counter(lst-ctn-kix_3yo7njaol6n6-2,decimal) ") "}.lst-kix_3yo7njaol6n6-1>li:before{content:"" counter(lst-ctn-kix_3yo7njaol6n6-1,lower-roman) ") "}.lst-kix_3yo7njaol6n6-3>li:before{content:"(" counter(lst-ctn-kix_3yo7njaol6n6-3,lower-latin) ") "}ol.lst-kix_3yo7njaol6n6-6.start{counter-reset:lst-ctn-kix_3yo7njaol6n6-6 0}.lst-kix_3yo7njaol6n6-0>li:before{content:"" counter(lst-ctn-kix_3yo7njaol6n6-0,lower-latin) ") "}ol{margin:0;padding:0}table td,table th{padding:0}.c8{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:line-through;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Times New Roman";font-style:italic}.c19{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Times New Roman";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c3{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c17{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c11{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c16{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c13{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c1{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c7{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c20{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{color:inherit;text-decoration:inherit}.c10{font-style:italic}.c9{height:11pt}.c4{font-weight:700}.c15{vertical-align:sub}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:11pt;font-family:"Times New Roman"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c20 doc-content"><h1 class="c16" id="h.jeqv0n3a6puh"><span class="c18">How Well Can GNNs Model the World?</span></h1><h3 class="c11" id="h.r1sidb3rw2l3"><span class="c3">Christy Li&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Gracie Sheng&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Claire Wang</span></h3><p class="c2"><span class="c7"><a class="c6" href="mailto:ckl@mit.edu">ckl@mit.edu</a></span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="c7"><a class="c6" href="mailto:grac@mit.edu">grac@mit.edu</a></span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c7"><a class="c6" href="mailto:clairely@mit.edu">clairely@mit.edu</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c1"><span class="c7"><a class="c6" href="#h.jfcrpfhzao2o">Introduction</a></span></p><p class="c1"><span class="c7"><a class="c6" href="#h.vuujay5q361i">Project Motivation</a></span></p><p class="c1"><span class="c7"><a class="c6" href="#h.go8r2ywqy7yu">Background and Related Work</a></span></p><p class="c1"><span class="c7"><a class="c6" href="#h.xufol0el8ec2">Methods</a></span></p><p class="c1"><span class="c7"><a class="c6" href="#h.h1enr8agkv13">Discussion and Conclusion</a></span></p><p class="c1"><span class="c7"><a class="c6" href="#h.89jc6sjrd2pp">Future Work</a></span></p><p class="c1"><span class="c7"><a class="c6" href="#h.mhk4rj93sjez">Works Cited</a></span></p><p class="c2 c9"><span class="c0"></span></p><h2 class="c5" id="h.jfcrpfhzao2o"><span>Introduction</span></h2><p class="c2"><span>E</span><span>ffective representation learning has become increasingly crucial for developing intelligent systems that can understand and interact with complex tasks and environments. A major challenge lies in extracting meaningful and structured representations of the world from high-dimensional observations. </span><span class="c10">Structured World Models</span><span class="c0">&nbsp;(SWMs) are a class of models that are used to learn abstract state representations from observations in an environment. These representations offer substantial advantages over holistic, unstructured approaches because they are able to learn object-centric representations and dynamics without explicit supervision.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>In our project, we revisited the GNN-based </span><span class="c10">Contrastively-trained SWM</span><span class="c0">&nbsp;(C-SWM) developed by Kipf et al. in [1] to investigate the architecture&#39;s scalability, generalization capabilities, and limitations in more complex scenarios. &nbsp;Our study presents a systematic investigation into the boundaries and failure modes of C-SWMs across three key dimensions: object-centric representation learning, physical complexity, and transfer learning. We extend the original work by testing the architecture on environments with varying numbers of objects, exploring its capacity to model increasingly complex n-body physics problems, and examining its potential for transfer learning between different Atari games. </span></p><h2 class="c5" id="h.vuujay5q361i"><span class="c12">Project Motivation</span></h2><p class="c2"><span class="c0">Object-centric representations have emerged as a key paradigm for understanding and modeling structured environments, especially for physical systems, where entities and their relationships are naturally separable. C-SWMs leverage object-centric representations combined with GNNs to model interactions between objects, using contrastive learning to capture underlying dynamics without requiring explicit labels [2]. This approach is particularly advantageous for environments where disentangling object-level features is critical for generalization, such as in multi-object physical simulations. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 154.67px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 154.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c4">Figure 1:</span><span class="c0">&nbsp;The C-SWM architecture consists of a CNN object extractor, an MLP object encoder, and a GNN transition model and uses contrastive loss. This example shows how the colored blocks in the 3D environment are transformed into abstract state representations by the C-SWM. Figure retrieved from original paper by Kipf et al. [1].</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>However, the reliance on accurate object detection and the absence of explicit mechanisms for modeling disentangled latent representations can limit the applicability of C-SWMs to complex, noisy, real-world scenarios. Addressing these limitations requires analyzing the interpretability of abstract state representations and identifying both success and failure modes of these models, which we cover in this project. Having reviewed related research summarized below, we found several endeavors which sought to improve the performance and functionality of SWMs. These were mainly &ldquo;black box&rdquo; experiments that compared metrics but did not explain the reasoning behind the model&rsquo;s behavior. In this project, our goal is to analyze </span><span class="c10">why.</span></p><h2 class="c5" id="h.go8r2ywqy7yu"><span class="c12">Background and Related Work</span></h2><p class="c2"><span>In this section, we provide context for the post-encoding portion of the C-SWM architecture, which is the central focus of our experiments.</span></p><h3 class="c11" id="h.7vnp71kwwfj7"><span class="c3">GNNs</span></h3><p class="c2"><span class="c10">Graph Neural Networks</span><span class="c0">&nbsp;(GNNs) are powerful for processing data with relational structure, such as social networks and molecular graphs, and constitute the abstract state transition model in C-SWMs. GNNs extend traditional neural networks by leveraging message-passing mechanisms to aggregate information from a node&rsquo;s neighbors, enabling them to capture both local and global relational patterns. The advent of the Graph Convolutional Network (GCN) exhibited the potential of GNNs in semi-supervised learning tasks on graph-structured data [2]. Subsequent advancements, like Graph Attention Networks (GATs) have refined the architecture to improve scalability and expressiveness [3][4]. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>Despite their strengths, GNNs have notable limitations. One primary drawback is their computational inefficiency on large-scale graphs, as message-passing schemes often require extensive memory and computation. To address this, sampling-based methods like GraphSAGE have been proposed to reduce resource overhead [5]. Another issue is the </span><span>oversmoothing</span><span class="c10">&nbsp;</span><span class="c0">problem, where repeated message passing can cause node representations to become indistinguishable, limiting the depth of GNNs. Efforts to mitigate this include architectural modifications like residual connections and improved aggregation functions [6]. Additionally, GNNs are inherently limited by their dependence on graph connectivity, which can lead to suboptimal performance in graphs with noisy or incomplete structures. These challenges underline the need for ongoing research to enhance the scalability, expressiveness, and robustness of GNN models.</span></p><h3 class="c11" id="h.4brmi9ssfywm"><span class="c3">Contrastive Learning</span></h3><p class="c2"><span class="c10">Contrastive learning</span><span class="c0">&nbsp;is a self-supervised technique that learns representations by contrasting similar and dissimilar samples, thereby extracting invariant features from raw observations. The contrastive training objective </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 343.11px; height: 66.38px;"><img alt="" src="images/image6.png" style="width: 343.11px; height: 66.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c10">N</span><span class="c0">: Number of pairs.</span></p><p class="c2"><span class="c10">y</span><span class="c10 c15">i</span><span class="c15">&nbsp;</span><span class="c0">: Binary label for similarity</span></p><p class="c2"><span class="c10">D</span><span class="c0">: Distance between embeddings</span></p><p class="c2"><span class="c10">m</span><span class="c0">: Margin</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">ensures that positive examples &mdash;actual transitions from the environment&mdash; are closer to one another in representation space, while negative examples &mdash;randomly paired states&mdash; are distanced apart. This loss formulation allows the C-SWM to learn an implicit structure of the environment and focus on meaningful object state transitions. The margin-based penalty helps the model to handle noise.</span></p><h3 class="c11" id="h.g7s1g02ekew6"><span>Structured World Models</span></h3><p class="c2"><span class="c10">World models</span><span>&nbsp;aim to learn an explicit representation of environment dynamics to improve sample efficiency, and allow agents to imagine or simulate potential future states and outcomes. World models were first officially introduced in Ha &amp; Schmidhuber 2018 [7]. In this paper, they relied on a VAE and training large RNN models to learn scene information and make changes to that scene. Typically, world models consist of a representation learning module to encode the observations, a dynamics model to predict future states, and a policy to select the best actions. </span><span class="c10">Structured world models</span><span>&nbsp;</span><span>leverage representation learning and relational dynamics to model complex environments by breaking them into entities and their interactions. It encodes objects as latent variables and uses mechanisms like graph neural networks to capture relationships, enabling better generalization to unseen configurations. Kipf et al. demonstrated the potential of C-SWMs in environments with clear object boundaries, highlighting their ability to infer object-centric representations in a self-supervised manner [1]. Furthermore, Collu et al. in 2023 introduced Slot Structured World Models (SSWM) to address limitations of previous approaches, particularly the inability of feedforward encoders to extract object-centric representations or disentangle multiple objects with similar appearances [8]. By combining Slot Attention-based object-centric encoders with latent graph-based dynamics models, SSWMs achieve superior performance in multi-step prediction tasks</span></p><h2 class="c5" id="h.xufol0el8ec2"><span>Methods</span></h2><p class="c2"><span>To evaluate the efficacy of C-SWMs, we reproduced the experiments outlined in the implementation by Kipf et al. [1], utilizing their publicly available repository (</span><span class="c7"><a class="c6" href="https://www.google.com/url?q=https://github.com/tkipf/c-swm&amp;sa=D&amp;source=editors&amp;ust=1733892170710200&amp;usg=AOvVaw0AIg61muGsvFnEYtBqtS-7">https://github.com/tkipf/c-swm</a></span><span>). The core experiments focused on assessing the ability of C-SWMs to learn object-centric representations and predict relational dynamics across various benchmarks. Prior to running new experiments, we first replicated the training pipeline, ensuring the alignment of parameters and experimental configurations with the original work. The primary datasets and environments used in the reproduction and in new experiments include multi-object interaction settings: </span><span class="c10">Atari Pong, Space Invaders, 3-Body Problem</span><span class="c0">.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>To modernize and extend the original implementation, we updated dependencies to work with current Python and library versions, resolving compatibility issues. Additionally, we introduced custom scripts to facilitate visualization of learned embeddings and relational dynamics, providing more intuitive insights into model performance. These updates are hosted in our forked repository (</span><span class="c7"><a class="c6" href="https://www.google.com/url?q=https://github.com/ClaireBookworm/scene-gnns&amp;sa=D&amp;source=editors&amp;ust=1733892170710510&amp;usg=AOvVaw2V5eUk5sq4p1g2el5WhnWE">https://github.com/ClaireBookworm/scene-gnns</a></span><span class="c0">). Our code for new experiments including inference and transfer learning are also present in the repository. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>To evaluate model performance, we employed standard metrics such as Hits @ 1 and Mean Reciprocal Rank (MRR), which quantify the accuracy of the model&#39;s object predictions and rank-based performance in relational reasoning tasks, respectively. These metrics provide a comprehensive assessment of the model&rsquo;s ability to generalize to unseen configurations and dynamics.</span></p><h2 class="c5" id="h.yimec256tcdb"><span class="c12">Experiments and Results</span></h2><h3 class="c11" id="h.mqqxzjgkb3fz"><span>Latent Representation Analysis</span></h3><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 158.67px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 158.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c4">Table 1</span><span>: Baselines of our ground state models in comparison to the original Kipf et al. 2020 paper results [1]. &ldquo;New&rdquo; represents our results and &ldquo;Orig.&rdquo; represents the paper&rsquo;s results. </span></p><h4 class="c17" id="h.wcuvoydqu0f"><span class="c19">Pong (K = 3, K = 5)</span></h4><p class="c2"><span class="c0">The first game experiment we reproduced was Atari Pong, which is a 2D interactive multi-object game. Like in the original paper, we used a dataset consisting of 50x50x6 tensor observations for training and evaluation, and trained for 200 epochs. Pong includes three objects: a ball and two blocks. We replicated experiments for K=3 and K=5 where K refers to the number of object slots the model will allocate. In the paper by Kipf et al., latent representations were not shown for complex game environments [1]. Hence, we visualized Pong in our experiments. See Figure 2 for the visualization of masks, embeddings, and state transitions for the Pong, K=5 experiment. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">A key finding in this experiment was that the encoder was able to capture all three game objects according to the object masks. Unfortunately, the model appeared to struggle with distinguishing between certain objects &mdash;see objects 3 and 5&mdash; and therefore treated two objects (in this case the ball and a block) as a single object. Nevertheless, it confidently discarded unnecessary object slots &mdash;see objects 1, 2, 4. The distance between similar and dissimilar objects in embedding space also aligns with the input data, which further indicates that the objects are being distinguished. This same observation is reflected in the per-object abstract state transition graphs. Since the state and action spaces of Pong are expansive, the interpretability of the state transition graphs appears to suffer, though the plots do imply smooth and continuous transitions over time, which remains in accordance with the dynamics of the game.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 647.00px; height: 589.00px;"><img alt="" src="images/image1.png" style="width: 654.00px; height: 589.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c4">Figure 2:</span><span class="c0">&nbsp;Atari Pong, K=5. The input, object masks, per-object abstract state transitions, and latent space object embeddings are depicted. Principal Component Analysis (PCA) was used to map latent representations onto the 2D plots.</span></p><h3 class="c11" id="h.gs6uidgdvlkf"><span class="c3">n-Body Problem</span></h3><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">We further investigate the C-SWM architecture&rsquo;s effectiveness at modeling physics scenarios. In [1], the authors report promising results for the model&rsquo;s ability to predict the next state of 3-body gravitational physics simulations. In contrast to the 2D-block and 3D-block grid worlds, there are no explicit actions in this environment. Instead, the model is provided with two frames of 50x50 pixel images that are consecutive in time and thus illustrate implicit action from the pair-wise gravitational forces between objects. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>Our first contribution is generalizing the 3-body physics simulation environment from [9], which requires that all objects have the same mass. Our new general simulation environment is able to model </span><span class="c10">n-body</span><span>&nbsp;gravitational physics between objects of </span><span class="c10">variable mass</span><span class="c0">. This way, we are able to explore more diverse physics environments, including those with objects that have distinct properties. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">For our experiments, we first reproduce the results from [1] with the 3-body environment and objects of equal mass. We also train C-SWM on a 3-body environment with objects of different masses to investigate if and how the model is able to represent systems in which &ldquo;actions&rdquo; depend on the properties of the objects (e.g. more or less massive objects in our physics simulation will follow different trajectories through space and time under the influence of the gravitational force from other objects). We hypothesize that such scenarios will be more difficult for the model to predict accurately, especially over many time steps. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">Additionally, we generate data from a 2-body environment with objects of the same mass and a 2-body environment with objects of different masses. We maintain training a C-SWM model with 3 object slots on the 2-body datasets to investigate how the model compensates for more object slots than the ground truth number of relevant objects to track. We hypothesize that the model will learn some kind of &ldquo;null&rdquo; object whose transitions are and position has no effect on the representations of &ldquo;real&rdquo; objects. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">Finally, we evaluate the performance of the 3-body, constant mass model on our new datasets (3-body with different masses, 2-body with the same masses, and 2-body with different masses) to investigate if the model has learned general physical laws that generalize to other physical environments. Our results are summarized in Table 2.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 617.50px; height: 414.90px;"><img alt="" src="images/image3.png" style="width: 628.60px; height: 457.33px; margin-left: -0.00px; margin-top: -26.90px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c4">Table 2</span><span>: </span><span class="c4">(a)</span><span>&nbsp;Ranking results from training the C-SWM model on data from a 3-body physics simulation with constant mass (mass ratio between objects is 1:1:1) and different masses (mass ratio between objects is 1:2:3) and from a 2-body physics simulation with constant mass (mass ratio between objects is 1:1) and different masses (mass ratio between objects is 1:2). </span><span class="c4">(b)</span><span class="c0">&nbsp;Ranking results from training the C-SWM model on data from a 3-body physics simulation with constant mass (mass ratio between objects is 1:1:1) and evaluating on each of the other physics scenarios.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">We found that all models perform near perfect in predicting the correct state after 1 step, however as the number of steps increases, performance falls off across all models. We see that in the long term, the model trained on the 2-body system with constant masses performs the best followed by the 3-body system with constant masses. This may be attributed to the fact that introducing objects of variable mass creates more complicated force relationships between objects for the model to learn. In evaluating the baseline 3-body system with constant masses on the datasets of the 3-body system with different masses, the 2-body system with constant masses, and the 2-body system with different masses, we find that it performs best on the 3-body system with different masses across all time steps. In all scenarios, but especially for the 2-body systems, the model&rsquo;s performance falls off drastically as the number of steps increases. This seems to imply that the model is not actually learning many generalizable laws of physics as much as it is simply identifying patterns within the specific setting it was trained on.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 205.33px;"><img alt="" src="images/image7.png" style="width: 624.00px; height: 205.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c4">Figure 3</span><span>: The object masks for each object learned by the models for </span><span class="c4">(a)</span><span>&nbsp;3-body system with constant mass </span><span class="c4">(b)</span><span>&nbsp;3-body system with mass ratio 1:2:3 </span><span class="c4">(c)</span><span>&nbsp;2-body system with constant mass </span><span class="c4">(d)</span><span class="c0">&nbsp;2-body system with mass ratio 1:2</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>Figure 3 shows visualizations of the object extractor learned object masks for each of the environments we tested. We see that, across all models, each &ldquo;object&rdquo; encoding does not actually correspond to any particular object but instead a far less interpretable representation of the image as a whole. Thus, when we trained C-SWM models with three object slots on data with only two relevant objects, we saw no major decrease in performance. Their encodings, as in the three object case, are not actually object-centric, so the model can easily compensate for these scenarios. This finding also </span><span class="c10">contradicts</span><span class="c0">&nbsp;a major claim in [1]: depending on the downstream task, the C-SWM architecture does not necessarily always find the object-centric embedding.</span></p><h3 class="c11" id="h.t1x99ul887u7"><span class="c3">Transfer Learning</span></h3><p class="c2"><span class="c0">We investigated the generalization capabilities of our Contrastively-trained Structured World Model (C-SWM) across different Atari game environments. Our experiments focus on cross-training between Space Invaders and Pong to understand the model&#39;s ability to transfer learned representations across disparate visual domains. We froze the object extractor and object encoder weights and only updated the transition model (GNN) weights. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">Our approach involves training the C-SWM on one game environment and fine-tuning it on another. This methodology allows us to probe the model&#39;s capacity for representation transfer and understand the underlying mechanisms of structural scene understanding.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">While task-specific performance showed limited transfer, we observed two critical insights: (1) The model demonstrated rapid convergence of latent representations during fine-tuning, suggesting an adaptable understanding of scene dynamics. (2) The inability to directly transfer performance highlights the complex challenges of cross-domain generalization in structured world models.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">Table 3 presents a tabular summary of the model&#39;s performance, measured by the 1-step, 5-step, and 10-step &ldquo;Hits@1 / MRR&quot; metrics. This data allows for a quantitative comparison of the model&#39;s capabilities across the different training configurations and in comparison to the original Space Invaders model. </span></p><p class="c2 c9"><span class="c4 c14"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 201.33px;"><img alt="" src="images/image8.png" style="width: 624.00px; height: 201.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c4">Table 3</span><span class="c0">: Ranking results from training the C-SWM model (k=3) on the Space Invaders dataset, comparing the performance of the original model that was trained for the task, the original Atari Pong model that has not been fine-tuned at all for the task, and the Atari Pong model fine-tuned for 50 and 100 epochs. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">Figure 4 provides a visual interpretation of the model&#39;s internal representations for each environment and training configuration. The model manages to learn a faithful representation of the new game environment. While the model may not achieve direct performance parity when transferred to a new task, the rapid adaptation of its latent representations suggests a possible more generalizable understanding of scene composition and relational dynamics.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c4">Figure 4:</span><span>&nbsp;Transfer-learned models, K=3. The input, object masks, and latent space object embeddings are depicted for the </span><span class="c4">(a)</span><span>&nbsp;</span><span>Pong model fine-tuned on the Space Invaders dataset and </span><span class="c4">(b)</span><span>&nbsp;Space Invaders model fine-tuned on the Pong dataset</span><span>&nbsp;for 50 and 100 epochs. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 294.67px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 294.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c5" id="h.h1enr8agkv13"><span>Discussion and </span><span class="c12">Conclusion</span></h2><p class="c2"><span>From our experiments, we understand that </span><span class="c0">C-SWMs perform well when applied to scenarios involving visually distinct objects and a fixed number of objects, where the model can effectively learn object relationships and representations. On the other hand, performance may degrade in more complex environments, such as games, where objects can vary greatly in appearance or number. This limitation arises from the model&rsquo;s reliance on a predefined number of objects, which is fixed during training. In terms of the parameter K, which represents the number of objects or &quot;nodes&quot; in the GNN, it plays a crucial role in the model&#39;s design. As seen in the Pong visualizations, the model learns to discard objects that do not exist in a given scene, effectively making use of only the relevant slots. Thus, the value of K dictates the capacity of the model to handle different numbers of objects, and it may be the case that not all slots are necessary for certain tasks, which could lead to inefficiencies. Future work could explore how to dynamically adjust K or implement more flexible representations to better accommodate variable object numbers in more complex scenes like games.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">The n-body experiments demonstrated the limitations of the C-SWM architecture in modeling increasingly complex physical systems. We see performance drop off significantly as the model tries to predict more time steps into the future or additional complications such as variable masses are added. Further, we found that the downstream task has a large effect on the interpretability of C-SWM embeddings. Although the 3-body and 2-body physics simulations involved only a small number of visually distinct objects, the encodings learned by the model were no longer object-centric.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span class="c0">While the fine-tuned models for Pong and Space Invaders did not perform better, more experiments must be done before we can make conclusions on the effects of transfer learning with C-SWMs. The GNN component of the C-SWM is likely a bottleneck for generalizing just between two different games since there is such a drastic change in the latent representations of the objects as well as game mechanics. Future work would be to compare the models&#39; performances on differing K values (e.g., K=1, K=5) and train it for more epochs. In addition, we fine-tuned the Atari Pong model on just 100 episodes of the Space Invaders dataset because of compute limits, but we could train it on the full 1000 episodes in the future. </span></p><h2 class="c5" id="h.89jc6sjrd2pp"><span class="c12">Future Work</span></h2><p class="c2"><span>Future research on C-SWMs could explore the integration of hard negative mining and the InfoNCE loss function to further improve contrastive learning. By focusing on more challenging negative examples during training by selecting multiple random examples and choosing the one that is the most distant from our current state, the model could learn to differentiate between similar objects more effectively. We had tested the difference in performance between InfoNCE in the Pong (K=3) model and saw only improvements when evaluating the next 10</span><span>&nbsp;steps and decreases in accuracy for 1 and 5 steps. However, we</span><span class="c0">&nbsp;did not train any further models, which means this could be a good starting point for future work. </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2"><span>Additionally, as noted in previous literature, it was observed that the model struggled with discerning </span><span class="c10">visually</span><span class="c0">&nbsp;similar objects, which could lead to confusion propagating to downstream tasks. To address this limitation, we propose investigating alternative encoder architectures, such as attention-based models or more advanced convolutional neural networks, that could better capture fine-grained visual features and improve the model&#39;s ability to distinguish between subtle differences in object appearance. These improvements could enhance the generalization capabilities of C-SWMs and make them more robust to a wider range of visual challenges, particularly on more complex datasets.</span></p><h2 class="c5" id="h.mhk4rj93sjez"><span class="c12">Works Cited</span></h2><p class="c2"><span class="c0">[1] Kipf, T., van der Pol, E., &amp; Welling, M. (2020). Contrastive Learning of Structured World Models. arXiv preprint arXiv:1911.12247.</span></p><p class="c2"><span class="c0">[2] Kipf, T. N., &amp; Welling, M. (2017). Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations (ICLR).</span></p><p class="c2"><span class="c0">[3] Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., &amp; Bengio, Y. (2018). Graph attention networks. International Conference on Learning Representations (ICLR).</span></p><p class="c2"><span class="c0">[4] Xu, K., Hu, W., Leskovec, J., &amp; Jegelka, S. (2019). How powerful are graph neural networks? International Conference on Learning Representations (ICLR).</span></p><p class="c2"><span class="c0">[5] Hamilton, W. L., Ying, Z., &amp; Leskovec, J. (2017). Inductive representation learning on large graphs. Advances in Neural Information Processing Systems (NeurIPS).</span></p><p class="c2"><span class="c0">[6] Li, Q., Han, Z., &amp; Wu, X.-M. (2018). Deeper insights into graph convolutional networks for semi-supervised learning. AAAI Conference on Artificial Intelligence.</span></p><p class="c2"><span>[7] Ha, D., &amp; Schmidhuber, J. (2018). World models. arXiv preprint arXiv:1803.10122. </span></p><p class="c2"><span>[8] Collu, C., Caselles-Dupr&eacute;, J., &amp; Houthooft, R. (2020). Slot-structured world models. </span><span class="c10">OpenReview.</span></p><p class="c2"><span class="c0">[9] Miguel Jaques, Michael Burke, and Timothy Hospedales. Physics-as-inverse-graphics: Joint unsupervised learning of objects and physics from video. arXiv preprint arXiv:1905.11169, 2019.</span></p><p class="c2"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; </span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2 c9"><span class="c0"></span></p><p class="c2 c9"><span class="c0"></span></p></body></html>